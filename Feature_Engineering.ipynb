{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvZvPNm5JM8qoR2c15HaBQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amithchintu/Feature-Engineering/blob/main/Feature_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What is a parameter?\n",
        "# A parameter refers to an internal configuration variable that is determined during the training process of a model. Parameters are essential components that help define the model's behavior and performance. Here's a closer look at what parameters are and their role in machine learning:\n",
        "\n",
        "# Key Points about Parameters\n",
        "# 1.Learned from Data:\n",
        "\n",
        "# Parameters are learned from the training data during the training phase of the model. They are adjusted by optimization algorithms to minimize the error or loss function and improve the model's performance.\n",
        "\n",
        "# 2.Types of Parameters:\n",
        "\n",
        "# Different types of machine learning models have different kinds of parameters. For instance:\n",
        "\n",
        "# Linear Regression: Parameters include the coefficients (weights) and the intercept.\n",
        "\n",
        "# Neural Networks: Parameters include weights and biases of the neurons.\n",
        "\n",
        "# Support Vector Machines: Parameters include the support vectors and the coefficients in the decision function.\n",
        "\n",
        "# 3.Role in Predictions:\n",
        "\n",
        "# Parameters directly influence the predictions made by the model. For example, in a linear regression model, the learned coefficients determine the slope and direction of the line that best fits the training data.\n",
        "\n",
        "# 4.Difference from Hyperparameters:\n",
        "\n",
        "# It’s important to differentiate between parameters and hyperparameters:\n",
        "\n",
        "# Parameters: Internally adjusted during training (e.g., weights in neural networks).\n",
        "\n",
        "# Hyperparameters: Set before the training process and control how the training is performed (e.g., learning rate, number of layers in a neural network)."
      ],
      "metadata": {
        "id": "6GCYNwmtSYtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([1.5, 3.1, 4.9, 6.8, 8.2])\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Parameters learned by the model\n",
        "print(\"Coefficient (Weight):\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjNpAh8mTxsY",
        "outputId": "793329eb-fb23-45a5-aaeb-66b2fc39b50c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficient (Weight): [1.71]\n",
            "Intercept: -0.22999999999999865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpretation\n",
        "# Coefficient (Weight): The parameter that determines the slope of the line.\n",
        "# Intercept: The parameter that determines the point where the line crosses the y-axis.\n",
        "\n",
        "# Summary\n",
        "# Parameters are crucial components of machine learning models that are learned from the training data. They define the model's behavior and influence its predictions. Understanding and optimizing parameters is essential for building effective and accurate models."
      ],
      "metadata": {
        "id": "dhagOKA1Tv47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. What is correlation? What does negative correlation mean?\n",
        "# Correlation in Machine Learning\n",
        "# Correlation is a statistical measure that describes the strength and direction of a relationship between two variables. In the context of machine learning, correlation is often used to understand how one variable might change in relation to another. It is quantified by the correlation coefficient, which ranges from -1 to 1:\n",
        "\n",
        "# +1 indicates a perfect positive correlation, where an increase in one variable is associated with an increase in the other.\n",
        "\n",
        "# -1 indicates a perfect negative correlation, where an increase in one variable is associated with a decrease in the other.\n",
        "\n",
        "# 0 indicates no correlation, meaning there is no linear relationship between the variables.\n",
        "\n",
        "# Types of Correlation\n",
        "\n",
        "# Positive Correlation:\n",
        "\n",
        "# When the value of one variable increases, the value of the other variable also increases.\n",
        "\n",
        "# Example: Height and weight tend to show a positive correlation because taller people often weigh more.\n",
        "\n",
        "# Negative Correlation:\n",
        "\n",
        "# When the value of one variable increases, the value of the other variable decreases.\n",
        "\n",
        "# Example: The speed of a car and the time to reach a destination show a negative correlation. As speed increases, the time to reach the destination decreases.\n",
        "\n",
        "# Negative Correlation Explained\n",
        "# Negative correlation means that as one variable increases, the other decreases, and vice versa. This inverse relationship is represented by a correlation coefficient that is less than 0 and greater than -1.\n",
        "\n",
        "# Example:\n",
        "# Consider a study where researchers are examining the relationship between the number of hours studied and the number of hours spent watching TV. They might find a negative correlation, indicating that as the number of study hours increases, the number of TV-watching hours decreases."
      ],
      "metadata": {
        "id": "NwySfxAfUVfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "hours_studied = np.array([1, 2, 3, 4, 5])\n",
        "hours_watching_tv = np.array([10, 8, 6, 5, 3])\n",
        "\n",
        "# Calculate correlation coefficient\n",
        "correlation_coefficient = np.corrcoef(hours_studied, hours_watching_tv)[0, 1]\n",
        "print(\"Correlation Coefficient:\", correlation_coefficient)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmLVtQLkVQAo",
        "outputId": "ef1a0ca2-5e68-427d-dd92-18f944b42260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation Coefficient: -0.9948497511671097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "# Correlation measures the relationship between two variables.\n",
        "# Negative Correlation indicates an inverse relationship, where an increase in one variable is associated with a decrease in another.\n",
        "\n",
        "# Understanding correlation is crucial in machine learning as it helps in feature selection and understanding the relationships between variables."
      ],
      "metadata": {
        "id": "-4z6GfCVVRwl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " # 3. Define Machine Learning. What are the main components in Machine Learning?\n",
        " # Definition of Machine Learning\n",
        " # Machine Learning (ML) is a subset of artificial intelligence (AI) that enables systems to learn and improve from experience without being explicitly programmed. It focuses on the development of algorithms and models that can analyze and interpret data, identify patterns, and make decisions with minimal human intervention.\n",
        " # Main Components in Machine Learning\n",
        " # Data:\n",
        "\n",
        " # Description: The foundation of any ML model. Quality and quantity of data significantly affect the model's performance.\n",
        "\n",
        " # Types: Structured (e.g., databases, spreadsheets) and Unstructured (e.g., text, images).\n",
        "\n",
        " # Features:\n",
        "\n",
        " # Description: Individual measurable properties or characteristics of the data used for training the model.\n",
        "\n",
        " # Feature Engineering: The process of selecting, transforming, and creating features to improve model performance.\n",
        "\n",
        " # Model:\n",
        "\n",
        " # Description: An algorithm or mathematical representation that learns patterns from the training data.\n",
        "\n",
        " # Types:\n",
        "\n",
        " # Supervised Learning: Models that learn from labeled data (e.g., linear regression, decision trees).\n",
        "\n",
        " # Unsupervised Learning: Models that find patterns in unlabeled data (e.g., clustering, dimensionality reduction).\n",
        "\n",
        " # Reinforcement Learning: Models that learn by interacting with the environment and receiving feedback (e.g., Q-learning).\n",
        "\n",
        "  # Training:\n",
        "\n",
        " # Description: The process of feeding the model with data and adjusting its parameters to minimize error.\n",
        "\n",
        " # Optimization: Techniques like gradient descent are used to adjust model parameters to improve accuracy.\n",
        "\n",
        " # Evaluation:\n",
        "\n",
        " # Description: Assessing the performance of the model using various metrics and techniques.\n",
        "\n",
        " # Metrics: Accuracy, precision, recall, F1-score, etc.\n",
        "\n",
        " # Validation Techniques: Cross-validation, train-test split, etc.\n",
        "\n",
        " # Hyperparameters:\n",
        "\n",
        " # Description: External settings used to control the training process, such as learning rate, batch size, and number of epochs.\n",
        "\n",
        " # Tuning: The process of selecting the best hyperparameters to optimize model performance.\n",
        "\n",
        " # Deployment:\n",
        "\n",
        " # Description: The process of integrating the trained model into a production environment where it can make predictions on new data.\n",
        "\n",
        " # Monitoring: Continuously assessing the model's performance in the real world and updating it as necessary."
      ],
      "metadata": {
        "id": "s8hPtkULVh_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Python code to demonstrate a simple machine learning workflow\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Sample data\n",
        "X = [[1], [2], [3], [4], [5]]  # Features\n",
        "y = [1.5, 3.1, 4.9, 6.8, 8.2]  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJRM8yRzVQ5s",
        "outputId": "32e35ddc-3861-41be-93ac-5e0b3ddb80a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.01653061224489807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "# Machine learning involves several key components: data, features, model, training, evaluation, hyperparameters, and deployment. Understanding these components and their interplay is essential for building effective and efficient machine learning models."
      ],
      "metadata": {
        "id": "IA3N9YN3WyzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.  How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "# The loss value is a key metric in machine learning that helps determine how well a model is performing. It quantifies the difference between the predicted values produced by the model and the actual values from the training data. Here’s how the loss value is useful in assessing the model's quality:\n",
        "\n",
        "# Role of Loss Value in Machine Learning\n",
        "# Indicator of Model Performance:\n",
        "\n",
        "# Definition: The loss function measures the error of the model's predictions. A lower loss value indicates better model performance.\n",
        "\n",
        "# Types of Loss Functions:\n",
        "\n",
        "# Mean Squared Error (MSE): Commonly used for regression tasks.\n",
        "\n",
        "# Cross-Entropy Loss: Often used for classification tasks.\n",
        "\n",
        "# Example: In a regression model, if the actual values are [1, 2, 3] and the predicted values are [1.1, 1.9, 3.2], the loss function will calculate the error between these sets of values.\n",
        "\n",
        "# Guiding Model Training:\n",
        "\n",
        "# Optimization: During training, the optimization algorithm (e.g., gradient descent) minimizes the loss function by adjusting the model's parameters (weights and biases).\n",
        "\n",
        "# Convergence: The goal is to reach a point where the loss value is as low as possible, indicating that the model predictions are close to the actual values.\n",
        "\n",
        "# Preventing Overfitting and Underfitting:\n",
        "\n",
        "# Overfitting: A model that performs very well on training data but poorly on test data may have a very low training loss but a high validation loss. Monitoring loss values helps identify overfitting.\n",
        "\n",
        "# Underfitting: A model that performs poorly on both training and test data has high loss values, indicating underfitting.\n",
        "\n",
        "# Model Comparison:\n",
        "\n",
        "# Benchmarking: Loss values can be used to compare different models. The model with the lowest loss on validation data is generally preferred.\n",
        "\n",
        "# Hyperparameter Tuning: Loss values guide the selection of optimal hyperparameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "MRYq_hQXW-rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([1.5, 3.1, 4.9, 6.8, 8.2])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test = X[:4], X[4:]\n",
        "y_train, y_test = y[:4], y[4:]\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate loss value (Mean Squared Error)\n",
        "loss_value = mean_squared_error(y_test, y_pred)\n",
        "print(\"Loss Value (MSE):\", loss_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vEmBFpLWumo",
        "outputId": "00f82d21-ea52-4614-a624-67f61e721f96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss Value (MSE): 0.09000000000000043\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpretation\n",
        "# Low Loss Value: Indicates that the model’s predictions are close to the actual values, suggesting good performance.\n",
        "\n",
        "# High Loss Value: Indicates that the model’s predictions deviate significantly from the actual values, suggesting poor performance.\n",
        "\n",
        "# Summary\n",
        "# The loss value is a crucial metric in machine learning that helps determine how well a model is performing. It guides the training process, helps in identifying overfitting or underfitting, and allows for the comparison of different models. By minimizing the loss value, we aim to improve the accuracy and reliability of the model's predictions."
      ],
      "metadata": {
        "id": "wcMymqLkXwI0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. What are continuous and categorical variables?\n",
        "# Continuous Variables\n",
        "# Continuous variables are numerical variables that can take an infinite number of values within a given range. They are measurable and can be divided into finer and finer levels of precision. These variables are often associated with quantities that can be measured rather than counted.\n",
        "\n",
        "# Examples:\n",
        "\n",
        "# Height: A person's height can be measured to any level of precision (e.g., 170.5 cm, 170.52 cm).\n",
        "\n",
        "# Temperature: The temperature in a room can be recorded to decimal places (e.g., 23.5°C, 23.57°C).\n",
        "\n",
        "# Characteristics:\n",
        "\n",
        "# Can take any value within a range.\n",
        "\n",
        "# Represent measurements.\n",
        "\n",
        "# Often include fractions and decimals.\n",
        "\n",
        "# Categorical Variables\n",
        "# Categorical variables, also known as qualitative variables, are variables that can take on a limited, fixed number of possible values, representing different categories or groups. They are not numerical and usually describe attributes or characteristics.\n",
        "\n",
        "# Examples:\n",
        "\n",
        "# Gender: Male, Female, Non-binary.\n",
        "\n",
        "# Color: Red, Blue, Green, Yellow.\n",
        "\n",
        "# Characteristics:\n",
        "\n",
        "# Take on discrete values.\n",
        "\n",
        "# Represent groups or categories.\n",
        "\n",
        "# Do not have an inherent order (in nominal categorical variables) or have a specific order (in ordinal categorical variables).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-o0jArXkX9w-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'Height': [160.5, 170.2, 165.3, 180.0, 175.8],  # Continuous variable\n",
        "    'Gender': ['Male', 'Female', 'Female', 'Male', 'Non-binary'],  # Categorical variable\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5Wubc-7XuRM",
        "outputId": "f7972814-3064-4d12-83f1-c32ef9b5c168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Height      Gender\n",
            "0   160.5        Male\n",
            "1   170.2      Female\n",
            "2   165.3      Female\n",
            "3   180.0        Male\n",
            "4   175.8  Non-binary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "# Continuous Variables: Numerical, measurable values that can take an infinite number of values within a range.\n",
        "\n",
        "# Categorical Variables: Qualitative, countable values representing different groups or categories.\n",
        "\n",
        "# Understanding the distinction between continuous and categorical variables is crucial in machine learning, as it influences how data is preprocessed, analyzed, and used to train models."
      ],
      "metadata": {
        "id": "U7wj2Y4WY1IN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "# Handling categorical variables is an essential part of preprocessing data for machine learning models. Here are some common techniques used to handle categorical variables:\n",
        "\n",
        "# 1. One-Hot Encoding\n",
        "# Description: Converts each category value into a new binary column (0 or 1). This is useful when the categorical variable does not have an inherent order (nominal variables).\n",
        "\n"
      ],
      "metadata": {
        "id": "F7_mx9U2Y8rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {'Color': ['Red', 'Blue', 'Green']}\n",
        "df = pd.DataFrame(data)\n",
        "one_hot_encoded = pd.get_dummies(df, columns=['Color'])\n",
        "print(one_hot_encoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbjbxIL5YxzN",
        "outputId": "7f4d58e3-693c-422b-a2f1-d7ccfcc46a5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Color_Blue  Color_Green  Color_Red\n",
            "0       False        False       True\n",
            "1        True        False      False\n",
            "2       False         True      False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Label Encoding\n",
        "# Description: Converts each category value into a unique integer. This is suitable for ordinal variables, where the order matters."
      ],
      "metadata": {
        "id": "v12eMhhPZrKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = ['Low', 'Medium', 'High']\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_data = label_encoder.fit_transform(data)\n",
        "print(encoded_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Vd5G6c0ZoO-",
        "outputId": "83476eea-dd6c-479e-b02c-612f5d2fcd3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Binary Encoding\n",
        "# Description: Combines the properties of label encoding and one-hot encoding. It first assigns a unique integer to each category, then converts those integers into binary code, and creates separate columns for each bit."
      ],
      "metadata": {
        "id": "FLKeqdeOZ4b1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Frequency Encoding\n",
        "# Description: Replaces each category with the frequency of its occurrence in the dataset. This is useful for high cardinality categorical variables."
      ],
      "metadata": {
        "id": "q0HzisLSaP8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = {'Category': ['A', 'B', 'A', 'C', 'B', 'A']}\n",
        "df = pd.DataFrame(data)\n",
        "frequency_encoded = df['Category'].map(df['Category'].value_counts())\n",
        "print(frequency_encoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSkga8EIaE8f",
        "outputId": "9124afe9-5853-4854-a375-568881c63e42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    3\n",
            "1    2\n",
            "2    3\n",
            "3    1\n",
            "4    2\n",
            "5    3\n",
            "Name: Category, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Target Encoding\n",
        "# Description: Replaces each category with the average value of the target variable for that category. This technique is useful for avoiding high-dimensionality problems and is typically used in supervised learning tasks."
      ],
      "metadata": {
        "id": "RSmieh64aZG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = {'Category': ['A', 'B', 'A', 'C', 'B', 'A'],\n",
        "        'Target': [10, 20, 30, 40, 20, 30]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate target mean for each category\n",
        "target_mean = df.groupby('Category')['Target'].mean()\n",
        "target_encoded = df['Category'].map(target_mean)\n",
        "print(target_encoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4Qdf_YJaUu6",
        "outputId": "8f1162ca-b3c1-48c1-93e2-d22b4b213ab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    23.333333\n",
            "1    20.000000\n",
            "2    23.333333\n",
            "3    40.000000\n",
            "4    20.000000\n",
            "5    23.333333\n",
            "Name: Category, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "# Handling categorical variables appropriately is crucial for building effective machine learning models. Techniques like one-hot encoding, label encoding, binary encoding, frequency encoding, and target encoding each have their strengths and use cases depending on the nature of the categorical data and the problem at hand."
      ],
      "metadata": {
        "id": "rLEvexDZaeL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.  What do you mean by training and testing a dataset?\n",
        "# Training and Testing a Dataset in Machine Learning\n",
        "# In machine learning, the concepts of training and testing datasets are fundamental to the process of building and evaluating models. Here’s a detailed explanation:\n",
        "\n",
        "# Training Dataset\n",
        "# Purpose:\n",
        "\n",
        "# The training dataset is used to train the machine learning model. It consists of input features and corresponding target values (labels).\n",
        "\n",
        "# The model learns the patterns and relationships in the training data to make accurate predictions.\n",
        "\n",
        "# Process:\n",
        "\n",
        "# The training process involves feeding the model with the training data multiple times (epochs), adjusting the model's parameters (weights and biases) to minimize the error or loss function.\n",
        "\n",
        "# Techniques like cross-validation, data augmentation, and regularization are often used to enhance the training process and prevent overfitting."
      ],
      "metadata": {
        "id": "3HbLMSYban-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample data\n",
        "X = [[1], [2], [3], [4], [5]]\n",
        "y = [1.5, 3.1, 4.9, 6.8, 8.2]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training Data (X_train):\", X_train)\n",
        "print(\"Training Labels (y_train):\", y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70N-FsONab9-",
        "outputId": "ccbb00f2-4e7d-4973-b72f-daf0a7d50c37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data (X_train): [[5], [3], [1], [4]]\n",
            "Training Labels (y_train): [8.2, 4.9, 1.5, 6.8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Dataset\n",
        "# Purpose:\n",
        "\n",
        "# The testing dataset is used to evaluate the performance of the trained model. It consists of input features and corresponding target values (labels) that the model has never seen during training.\n",
        "\n",
        "# The goal is to assess how well the model generalizes to new, unseen data.\n",
        "\n",
        "# Process:\n",
        "\n",
        "# After the model is trained, it is tested on the testing dataset to calculate performance metrics such as accuracy, precision, recall, F1-score, mean squared error, etc.\n",
        "\n",
        "# These metrics help determine the model's effectiveness and identify any issues like overfitting or underfitting."
      ],
      "metadata": {
        "id": "BB-XOEVCbdzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Testing Data (X_test):\", X_test)\n",
        "print(\"True Labels (y_test):\", y_test)\n",
        "print(\"Predicted Labels (y_pred):\", y_pred)\n",
        "print(\"Mean Squared Error on Testing Data:\", mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4KDPiowbTjo",
        "outputId": "207ff652-153d-4e5a-fc63-4c9639735c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Data (X_test): [[2]]\n",
            "True Labels (y_test): [3.1]\n",
            "Predicted Labels (y_pred): [3.22857143]\n",
            "Mean Squared Error on Testing Data: 0.01653061224489807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "# Training Dataset: Used to train the model, allowing it to learn patterns and relationships.\n",
        "\n",
        "# Testing Dataset: Used to evaluate the model's performance and generalization to new data.\n",
        "\n",
        "# These datasets are crucial for building robust and reliable machine learning models. Properly splitting the data and evaluating the model's performance helps ensure that the model is both accurate and capable of making predictions on new, unseen data."
      ],
      "metadata": {
        "id": "9TLQiRkMbyt8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. What is sklearn.preprocessing?\n",
        "# sklearn.preprocessing is a module in the scikit-learn library that provides a variety of functions and classes for preprocessing data. Preprocessing is a crucial step in the machine learning workflow, as it involves transforming raw data into a suitable format for model training. Proper preprocessing can significantly improve model performance and accuracy.\n",
        "# Key Features of sklearn.preprocessing\n",
        "# Standardization and Normalization:\n",
        "\n",
        "# StandardScaler: Standardizes features by removing the mean and scaling to unit variance.\n",
        "\n",
        "# MinMaxScaler: Scales features to a given range, typically between 0 and 1.\n",
        "\n",
        "# Normalizer: Normalizes samples individually to unit norm.\n",
        "\n",
        "# Encoding Categorical Features:\n",
        "\n",
        "# LabelEncoder: Converts categorical labels into integer codes.\n",
        "\n",
        "# OneHotEncoder: Encodes categorical integer features as a one-hot numeric array.\n",
        "\n",
        "# Binarization:\n",
        "\n",
        "# Binarizer: Converts continuous data into binary values based on a threshold.\n",
        "\n",
        "# Polynomial Features:\n",
        "\n",
        "# PolynomialFeatures: Generates polynomial and interaction features from the original features.\n",
        "\n",
        "# Handling Missing Values:\n",
        "\n",
        "# Imputer: Fills in missing values using specified strategies (e.g., mean, median).\n",
        "\n",
        "# Feature Scaling:\n",
        "\n",
        "# RobustScaler: Scales features using statistics that are robust to outliers."
      ],
      "metadata": {
        "id": "EioTryi7cBKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "# The sklearn.preprocessing module in scikit-learn provides essential tools for transforming and preparing data for machine learning models. By standardizing, encoding, binarizing, and generating new features, you can enhance the quality of your dataset and improve model performance."
      ],
      "metadata": {
        "id": "ZyztLguddyRk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. What is a Test set?\n",
        "\n",
        "# A test set in machine learning is a subset of the dataset used to evaluate the performance of a trained model. It contains data that the model has not seen during the training phase, which helps assess how well the model generalizes to new, unseen data. Here's a closer look at its role and importance:\n",
        "\n",
        "# Key Points about the Test Set\n",
        "# Purpose:\n",
        "\n",
        "# Model Evaluation: The primary purpose of the test set is to provide an unbiased evaluation of a model's performance. It helps determine how well the model can predict outcomes on new data.\n",
        "\n",
        "# Generalization: By testing the model on data it hasn't seen before, we can get an estimate of how well the model will perform in real-world scenarios.\n",
        "\n",
        "# Separation from Training Set:\n",
        "\n",
        "# During the data preparation process, the dataset is typically split into two (or more) parts: the training set and the test set.\n",
        "\n",
        "# Commonly, the data is split into 70-80% for training and 20-30% for testing, though this ratio can vary depending on the dataset size and specific requirements.\n",
        "\n",
        "# Metrics:\n",
        "\n",
        "# The test set is used to calculate performance metrics such as accuracy, precision, recall, F1-score, mean squared error, and others. These metrics help quantify the model's predictive power and identify potential issues like overfitting or underfitting.\n",
        "\n",
        "# Overfitting and Underfitting:\n",
        "\n",
        "# Overfitting: When a model performs very well on the training set but poorly on the test set, it may indicate that the model has overfitted to the training data. Overfitting means the model has learned the noise and specific patterns in the training data that do not generalize to new data.\n",
        "\n",
        "# Underfitting: When a model performs poorly on both the training and test sets, it suggests that the model is too simple to capture the underlying patterns in the data.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FIYj-Y17eB_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([1.5, 3.1, 4.9, 6.8, 8.2])\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Test Set Predictions:\", y_pred)\n",
        "print(\"Mean Squared Error on Test Set:\", mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_8ORJ58dSt7",
        "outputId": "ab4c9904-9872-43d6-b365-eef587ac6297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set Predictions: [3.22857143]\n",
            "Mean Squared Error on Test Set: 0.01653061224489807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "# The test set is crucial for evaluating the performance and generalization capability of a machine learning model. By assessing the model's performance on unseen data, we can gain insights into its real-world effectiveness and identify areas for improvement. Properly splitting and using the test set ensures that we build robust and reliable models.\n"
      ],
      "metadata": {
        "id": "K8mBoW9_fhvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. How do we split data for model fitting (training and testing) in Python?How do you approach a Machine Learning problem?\n",
        "# Splitting data for model fitting and approaching a machine learning problem involves a structured process to ensure that the model is trained effectively and evaluated accurately. Here’s a detailed guide to both aspects:\n",
        "# 1. Splitting Data for Model Fitting\n",
        "# To split data into training and testing sets in Python, you can use the train_test_split function from the sklearn.model_selection module. This function is widely used for splitting data into training and test sets.\n",
        "\n"
      ],
      "metadata": {
        "id": "v9GT2yiCfpcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
        "y = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training data (X_train):\", X_train)\n",
        "print(\"Testing data (X_test):\", X_test)\n",
        "print(\"Training labels (y_train):\", y_train)\n",
        "print(\"Testing labels (y_test):\", y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVoHhQMSfcLi",
        "outputId": "5db6d285-6771-4729-8be2-fcdac22670a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data (X_train): [[ 6]\n",
            " [ 1]\n",
            " [ 8]\n",
            " [ 3]\n",
            " [10]\n",
            " [ 5]\n",
            " [ 4]\n",
            " [ 7]]\n",
            "Testing data (X_test): [[9]\n",
            " [2]]\n",
            "Training labels (y_train): [ 60  10  80  30 100  50  40  70]\n",
            "Testing labels (y_test): [90 20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "# X: Input features.\n",
        "\n",
        "# y: Target variable.\n",
        "\n",
        "# test_size: Proportion of the dataset to include in the test split (e.g., 0.2 means 20% of the data is used for testing).\n",
        "\n",
        "# random_state: Seed used by the random number generator to ensure reproducibility.\n",
        "\n",
        "# 2. Approach to a Machine Learning Problem\n",
        "# Step-by-Step Process:\n",
        "# Problem Definition:\n",
        "\n",
        "# Clearly define the problem you are trying to solve. Understand the goal, whether it’s classification, regression, clustering, etc.\n",
        "\n",
        "# Data Collection:\n",
        "\n",
        "# Gather relevant data from various sources. Ensure the data is representative of the problem you are solving.\n",
        "\n",
        "# Data Preprocessing:\n",
        "\n",
        "# Cleaning: Handle missing values, remove duplicates, and correct errors.\n",
        "\n",
        "# Transformation: Scale/normalize data, encode categorical variables, and create new features if necessary.\n",
        "\n",
        "# Exploratory Data Analysis (EDA):\n",
        "\n",
        "# Visualize and analyze the data to understand distributions, relationships, and patterns. Use plots like histograms, scatter plots, and box plots.\n",
        "\n",
        "# Feature Selection:\n",
        "\n",
        "# Select the most relevant features that contribute to the target variable. Use techniques like correlation analysis, feature importance scores, and domain knowledge.\n",
        "\n",
        "# Model Selection:\n",
        "\n",
        "# Choose appropriate machine learning algorithms based on the problem type and data characteristics. Consider using models like linear regression, decision trees, SVM, neural networks, etc.\n",
        "\n",
        "# Model Training:\n",
        "\n",
        "# Split the data into training and testing sets. Train the model on the training data and adjust hyperparameters to optimize performance.\n",
        "\n",
        "# Model Evaluation:\n",
        "\n",
        "# Evaluate the model’s performance using the test set. Calculate metrics like accuracy, precision, recall, F1-score, mean squared error, etc.\n",
        "\n",
        "# Model Tuning:\n",
        "\n",
        "# Fine-tune the model by adjusting hyperparameters and potentially trying different algorithms. Use techniques like cross-validation and grid search.\n",
        "\n",
        "# Model Deployment:\n",
        "\n",
        "# Deploy the model to a production environment where it can make predictions on new, unseen data. Monitor the model’s performance and update it as needed.\n"
      ],
      "metadata": {
        "id": "M03i3gYmhBW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
        "y = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error on Test Set:\", mse)\n",
        "\n",
        "# Plot results\n",
        "plt.scatter(X_test, y_test, color='blue', label='Actual')\n",
        "plt.scatter(X_test, y_pred, color='red', label='Predicted')\n",
        "plt.legend()\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Actual vs Predicted')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "LtPBx3mgg22Q",
        "outputId": "84205daf-7459-41b0-a5c0-acecd1fb3bc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error on Test Set: 6.310887241768095e-30\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "# Splitting data into training and testing sets and following a structured approach to a machine learning problem are crucial steps for building effective models. Proper data preprocessing, feature selection, and model evaluation ensure that the model generalizes well to new data and performs accurately in real-world scenarios."
      ],
      "metadata": {
        "id": "-OqdSvdejgmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Why do we have to perform EDA before fitting a model to the data?\n",
        "# Exploratory Data Analysis (EDA) is a crucial step in the machine learning workflow. It involves analyzing and visualizing data to understand its main characteristics before applying any machine learning algorithms. Here are some compelling reasons why EDA is essential before fitting a model to the data:\n",
        "\n",
        "# Reasons for Performing EDA\n",
        "# Understanding Data Distribution:\n",
        "\n",
        "# EDA helps in understanding the distribution of data, such as identifying skewness, outliers, and the overall spread of the data. This information is vital for selecting appropriate preprocessing techniques and models.\n",
        "\n",
        "# Identifying Outliers and Anomalies:\n",
        "\n",
        "# Outliers can significantly impact model performance. EDA helps in detecting and deciding how to handle these outliers to ensure they do not adversely affect the model.\n",
        "\n",
        "# Detecting Missing Values:\n",
        "\n",
        "# Missing data can lead to biased results and poor model performance. EDA helps in identifying missing values and deciding on strategies for handling them, such as imputation or removal.\n",
        "\n",
        "# Feature Relationships:\n",
        "\n",
        "# EDA allows you to explore relationships between features and the target variable. Understanding these relationships can guide feature selection, engineering, and the choice of modeling techniques.\n",
        "\n",
        "# Data Quality Issues:\n",
        "\n",
        "# EDA helps in detecting data quality issues such as inconsistencies, duplicate entries, and data entry errors. Addressing these issues is crucial for building a reliable model.\n",
        "\n",
        "# Informing Feature Engineering:\n",
        "\n",
        "# Insights gained from EDA can guide feature engineering efforts, such as creating new features, transforming existing ones, and selecting the most relevant features for modeling.\n",
        "\n",
        "# Choosing the Right Model:\n",
        "\n",
        "# The insights from EDA can help in selecting the most appropriate machine learning algorithms. For example, if features are highly correlated, regularization techniques might be necessary.\n",
        "\n",
        "# Checking Assumptions:\n",
        "\n",
        "# Many machine learning algorithms have underlying assumptions about the data (e.g., normality, linearity). EDA helps in checking these assumptions and making necessary adjustments.\n",
        "\n"
      ],
      "metadata": {
        "id": "3iKZvCifjsMz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "# Performing EDA is a critical step before fitting a machine learning model to the data. It helps in understanding the data, detecting potential issues, and guiding feature engineering and model selection. By thoroughly exploring the data, you can build more reliable and accurate machine learning models."
      ],
      "metadata": {
        "id": "AmrikhG8krO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. What is correlation?\n",
        "# Correlation is a statistical measure that describes the strength and direction of a relationship between two variables. It quantifies how much two variables change together. Correlation is represented by a correlation coefficient, which ranges from -1 to 1.\n",
        "\n",
        "# Key Points about Correlation:\n",
        "# Correlation Coefficient:\n",
        "\n",
        "# +1: Perfect positive correlation. As one variable increases, the other also increases proportionally.\n",
        "\n",
        "# 0: No correlation. There is no linear relationship between the variables.\n",
        "\n",
        "# -1: Perfect negative correlation. As one variable increases, the other decreases proportionally.\n",
        "\n",
        "# Types of Correlation:\n",
        "\n",
        "# Positive Correlation: Both variables move in the same direction.\n",
        "\n",
        "# Negative Correlation: The variables move in opposite directions.\n",
        "\n",
        "# Zero Correlation: No linear relationship between the variables.\n",
        "\n",
        "# Example of Correlation:\n",
        "# Consider the following example:\n",
        "\n",
        "# Height and Weight: Generally, taller people tend to weigh more, so height and weight have a positive correlation.\n",
        "\n",
        "# Temperature and Heating Bills: As temperature increases, heating bills typically decrease, showing a negative correlation."
      ],
      "metadata": {
        "id": "XNBwOkY5k4Ta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "height = [150, 160, 170, 180, 190]\n",
        "weight = [50, 60, 70, 80, 90]\n",
        "\n",
        "# Calculate the correlation coefficient\n",
        "correlation_coefficient = np.corrcoef(height, weight)[0, 1]\n",
        "print(\"Correlation Coefficient:\", correlation_coefficient)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0WWPc1Vkn4b",
        "outputId": "2a5e6bab-f4d1-4c67-dd54-765e362d8225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation Coefficient: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpretation:\n",
        "# Positive Correlation: A correlation coefficient greater than 0 indicates that as one variable increases, the other also increases.\n",
        "\n",
        "# Negative Correlation: A correlation coefficient less than 0 indicates that as one variable increases, the other decreases.\n",
        "\n",
        "# Strength: The closer the correlation coefficient is to ±1, the stronger the relationship.\n",
        "\n",
        "# Summary:\n",
        "# Correlation helps in understanding the relationship between two variables and can be crucial for feature selection in machine learning. By analyzing the correlation, you can gain insights into how variables influence each other and use that information to make informed decisions."
      ],
      "metadata": {
        "id": "6TCBDQdmlnx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. What does negative correlation mean?\n",
        "# Negative correlation describes an inverse relationship between two variables. When one variable increases, the other tends to decrease, and vice versa. The correlation coefficient for a negative correlation ranges from -1 to 0. Here are some key points about negative correlation:\n",
        "\n",
        "# Key Characteristics of Negative Correlation\n",
        "# Inverse Relationship:\n",
        "\n",
        "# As one variable increases, the other decreases. Conversely, when one variable decreases, the other increases.\n",
        "\n",
        "# Correlation Coefficient:\n",
        "\n",
        "# The correlation coefficient for a perfect negative correlation is -1. This indicates a strong inverse relationship.\n",
        "\n",
        "# Values closer to -1 indicate a stronger negative correlation, while values closer to 0 indicate a weaker negative correlation.\n",
        "\n",
        "# Examples:\n",
        "\n",
        "# Temperature and Heating Bills: As the temperature increases, heating bills typically decrease, showing a negative correlation.\n",
        "\n",
        "# Hours of Exercise and Body Weight: Generally, as the number of hours spent exercising increases, body weight tends to decrease, indicating a negative correlation.\n"
      ],
      "metadata": {
        "id": "1I8QUVpQlvmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "hours_exercised = [1, 2, 3, 4, 5]\n",
        "body_weight = [80, 78, 75, 73, 70]\n",
        "\n",
        "# Calculate the correlation coefficient\n",
        "correlation_coefficient = np.corrcoef(hours_exercised, body_weight)[0, 1]\n",
        "print(\"Correlation Coefficient:\", correlation_coefficient)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Dtejmp4lg9c",
        "outputId": "7bdad71e-ef34-4820-cf65-6970efc5bb12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation Coefficient: -0.9976086055845277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "# Negative correlation means that as one variable increases, the other decreases. Understanding negative correlations is important in many fields, including finance, healthcare, and environmental science, as it helps identify and interpret inverse relationships between variables."
      ],
      "metadata": {
        "id": "2qfYGFF6mrLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14. How can you find correlation between variables in Python?\n",
        "# Finding the correlation between variables in Python can be done using a few different methods, depending on the specific requirements of your analysis. Here are some common approaches:\n",
        "\n",
        "# 1. Using numpy.corrcoef\n",
        "# This function calculates the Pearson correlation coefficient matrix.\n"
      ],
      "metadata": {
        "id": "HUhhZlMumu6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "x = [1, 2, 3, 4, 5]\n",
        "y = [10, 20, 30, 40, 50]\n",
        "\n",
        "# Calculate the correlation coefficient matrix\n",
        "correlation_matrix = np.corrcoef(x, y)\n",
        "correlation_coefficient = correlation_matrix[0, 1]\n",
        "print(\"Correlation Coefficient:\", correlation_coefficient)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0y94PVpmihq",
        "outputId": "1bd45f2e-8b98-426b-c024-7f2a40d039f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation Coefficient: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Using pandas.DataFrame.corr\n",
        "# If you are working with data in a DataFrame, you can use the corr method to calculate the correlation matrix for all numerical columns."
      ],
      "metadata": {
        "id": "3aG1wL2do5Lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = {'x': [1, 2, 3, 4, 5],\n",
        "        'y': [10, 20, 30, 40, 50]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "print(\"Correlation Matrix:\\n\", correlation_matrix)\n",
        "\n",
        "# Extract the correlation coefficient between x and y\n",
        "correlation_coefficient = correlation_matrix.loc['x', 'y']\n",
        "print(\"Correlation Coefficient between x and y:\", correlation_coefficient)\n"
      ],
      "metadata": {
        "id": "zeraCIlUnQFB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2cc654b-9977-41d1-dc01-33431e207c30"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation Matrix:\n",
            "      x    y\n",
            "x  1.0  1.0\n",
            "y  1.0  1.0\n",
            "Correlation Coefficient between x and y: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Using scipy.stats.pearsonr\n",
        "# The pearsonr function calculates the Pearson correlation coefficient along with a p-value to test the null hypothesis of no correlation."
      ],
      "metadata": {
        "id": "GZTLpn1ppCMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Sample data\n",
        "x = [1, 2, 3, 4, 5]\n",
        "y = [10, 20, 30, 40, 50]\n",
        "\n",
        "# Calculate Pearson correlation coefficient and p-value\n",
        "correlation_coefficient, p_value = pearsonr(x, y)\n",
        "print(\"Pearson Correlation Coefficient:\", correlation_coefficient)\n",
        "print(\"P-value:\", p_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCr4xIIDo_CI",
        "outputId": "0f021bad-a8d6-4596-bbc9-020576734c0f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pearson Correlation Coefficient: 1.0\n",
            "P-value: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "# These methods allow you to calculate the correlation between variables in Python easily. Depending on your needs and the structure of your data, you can use numpy, pandas, or scipy to perform the correlation analysis and gain insights into the relationships between variables."
      ],
      "metadata": {
        "id": "v3QahTGRpMDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15. What is causation? Explain difference between correlation and causation with an example.\n",
        "# Causation\n",
        "# Causation refers to a relationship between two variables where one variable directly affects the other. In other words, changes in one variable (the cause) lead to changes in another variable (the effect). Establishing causation implies that there is a cause-and-effect relationship between the variables.\n",
        "\n",
        "# Correlation vs. Causation\n",
        "# Correlation: Indicates that there is a statistical association between two variables. It shows that the variables tend to change together, but it does not imply that one variable causes the change in the other.\n",
        "\n",
        "# Causation: Indicates that one variable directly influences or causes a change in another variable. Establishing causation requires more rigorous evidence than simply observing correlation.\n",
        "\n",
        "# Example\n",
        "# Correlation Example: Suppose you find a positive correlation between ice cream sales and drowning incidents. This means that when ice cream sales increase, the number of drowning incidents also increases. However, this correlation does not imply that eating ice cream causes drowning.\n",
        "\n",
        "# Causation Example: Consider the relationship between smoking and lung cancer. Extensive research and evidence have shown that smoking causes lung cancer. This is a causal relationship because smoking directly leads to the development of lung cancer.\n",
        "\n",
        "# Summary\n",
        "# While correlation can indicate a potential relationship between two variables, it does not prove causation. Establishing causation requires rigorous experimentation and evidence to demonstrate that one variable directly affects another."
      ],
      "metadata": {
        "id": "-Pg3yNrPpW8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "# Optimizer in Machine Learning\n",
        "# An optimizer is an algorithm or method used to adjust the parameters of a machine learning model to minimize the loss function (or cost function) during training. The goal of an optimizer is to find the optimal set of parameters that lead to the best performance of the model. Optimizers play a crucial role in the convergence and efficiency of training machine learning models.\n",
        "\n",
        "# Types of Optimizers\n",
        "# Gradient Descent\n",
        "\n",
        "# Description: The most basic optimization algorithm that iteratively updates the model parameters in the direction of the negative gradient of the loss function.\n",
        "\n",
        "# Variants:\n",
        "\n",
        "# Batch Gradient Descent: Uses the entire dataset to compute gradients and update parameters.\n",
        "\n",
        "# Stochastic Gradient Descent (SGD): Uses one sample at a time to update parameters.\n",
        "\n",
        "# Mini-Batch Gradient Descent: Uses a small batch of samples to compute gradients and update parameters."
      ],
      "metadata": {
        "id": "tzAVAWzcp7tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Simple gradient descent example for linear regression\n",
        "def gradient_descent(X, y, learning_rate, epochs):\n",
        "    m = len(y)\n",
        "    theta = np.zeros(X.shape[1])\n",
        "    for _ in range(epochs):\n",
        "        gradient = (1/m) * np.dot(X.T, (np.dot(X, theta) - y))\n",
        "        theta -= learning_rate * gradient\n",
        "    return theta\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
        "y = np.array([6, 8, 9, 11])\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "# Train model\n",
        "theta = gradient_descent(X, y, learning_rate, epochs)\n",
        "print(\"Optimal Parameters:\", theta)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGKnOgpDpIB_",
        "outputId": "e45b8057-d8d8-425e-8828-bba76b998280"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Parameters: [2.04054768 2.58265021]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Momentum\n",
        "\n",
        "# Description: An extension of gradient descent that accelerates convergence by adding a fraction of the previous update to the current update, helping to dampen oscillations."
      ],
      "metadata": {
        "id": "pmroHAHCqjl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def momentum(X, y, learning_rate, epochs, beta):\n",
        "    m = len(y)\n",
        "    theta = np.zeros(X.shape[1])\n",
        "    v = np.zeros(X.shape[1])\n",
        "    for _ in range(epochs):\n",
        "        gradient = (1/m) * np.dot(X.T, (np.dot(X, theta) - y))\n",
        "        v = beta * v + learning_rate * gradient\n",
        "        theta -= v\n",
        "    return theta\n"
      ],
      "metadata": {
        "id": "v2epN1ogqftr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adagrad (Adaptive Gradient)\n",
        "\n",
        "# Description: An optimizer that adapts the learning rate for each parameter based on the historical gradients, making larger updates for infrequent parameters and smaller updates for frequent parameters."
      ],
      "metadata": {
        "id": "1n_HvFiSqrEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adagrad(X, y, learning_rate, epochs, epsilon=1e-8):\n",
        "    m = len(y)\n",
        "    theta = np.zeros(X.shape[1])\n",
        "    G = np.zeros(X.shape[1])\n",
        "    for _ in range(epochs):\n",
        "        gradient = (1/m) * np.dot(X.T, (np.dot(X, theta) - y))\n",
        "        G += gradient**2\n",
        "        adjusted_gradient = gradient / (np.sqrt(G) + epsilon)\n",
        "        theta -= learning_rate * adjusted_gradient\n",
        "    return theta\n"
      ],
      "metadata": {
        "id": "nkWcCKoIqn5m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RMSprop (Root Mean Square Propagation)\n",
        "\n",
        "# Description: An optimizer that adapts the learning rate for each parameter by dividing the gradient by a moving average of recent gradients' magnitudes.\n"
      ],
      "metadata": {
        "id": "ekVxetFGq4lU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rmsprop(X, y, learning_rate, epochs, beta=0.9, epsilon=1e-8):\n",
        "    m = len(y)\n",
        "    theta = np.zeros(X.shape[1])\n",
        "    E_g2 = np.zeros(X.shape[1])\n",
        "    for _ in range(epochs):\n",
        "        gradient = (1/m) * np.dot(X.T, (np.dot(X, theta) - y))\n",
        "        E_g2 = beta * E_g2 + (1 - beta) * gradient**2\n",
        "        adjusted_gradient = gradient / (np.sqrt(E_g2) + epsilon)\n",
        "        theta -= learning_rate * adjusted_gradient\n",
        "    return theta\n"
      ],
      "metadata": {
        "id": "BI6tuiG7qvyz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adam (Adaptive Moment Estimation)\n",
        "\n",
        "# Description: An optimizer that combines the benefits of both Momentum and RMSprop. It maintains two moving averages: one for the gradients and another for the squared gradients."
      ],
      "metadata": {
        "id": "LxlJaIOLrHZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adam(X, y, learning_rate, epochs, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "    m = len(y)\n",
        "    theta = np.zeros(X.shape[1])\n",
        "    m_t = np.zeros(X.shape[1])\n",
        "    v_t = np.zeros(X.shape[1])\n",
        "    for t in range(1, epochs + 1):\n",
        "        gradient = (1/m) * np.dot(X.T, (np.dot(X, theta) - y))\n",
        "        m_t = beta1 * m_t + (1 - beta1) * gradient\n",
        "        v_t = beta2 * v_t + (1 - beta2) * gradient**2\n",
        "        m_hat = m_t / (1 - beta1**t)\n",
        "        v_hat = v_t / (1 - beta2**t)\n",
        "        theta -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
        "    return theta\n"
      ],
      "metadata": {
        "id": "tWMngMXwrCTv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "# Optimizers are essential components in machine learning that help in adjusting model parameters to minimize the loss function. Different optimizers, such as Gradient Descent, Momentum, Adagrad, RMSprop, and Adam, have unique mechanisms for updating parameters and improving model convergence. Selecting the right optimizer depends on the specific problem and dataset characteristics."
      ],
      "metadata": {
        "id": "r7FXE731rPJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 17. What is sklearn.linear_model ?\n",
        "# sklearn.linear_model is a module within the scikit-learn library in Python that provides a variety of linear models for regression and classification. These models are based on linear relationships between the input features and the target variable. The module includes several popular algorithms, each suitable for different types of data and problems.\n",
        "\n",
        "# Key Models in sklearn.linear_model\n",
        "# LinearRegression:\n",
        "\n",
        "# Description: A standard linear regression model that fits a linear relationship between the input features and the target variable by minimizing the residual sum of squares.\n"
      ],
      "metadata": {
        "id": "w6xB0WV5rZqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data\n",
        "X = [[1], [2], [3], [4], [5]]\n",
        "y = [1.5, 3.1, 4.9, 6.8, 8.2]\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict([[6], [7]])\n",
        "print(\"Predictions:\", y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tltP9i7nrMnM",
        "outputId": "20cf2b1f-6acd-4b14-f931-15adc0513d7b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [10.03 11.74]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ridge:\n",
        "\n",
        "# Description: A linear regression model with L2 regularization, which helps to prevent overfitting by adding a penalty proportional to the squared magnitude of the coefficients."
      ],
      "metadata": {
        "id": "xvXCVlkor5A0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "model = Ridge(alpha=1.0)\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict([[6], [7]])\n",
        "print(\"Predictions:\", y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkH2JCHFrzvV",
        "outputId": "33b8937c-47be-4674-ee82-02513b19d35a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [ 9.56363636 11.11818182]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lasso:\n",
        "\n",
        "# Description: A linear regression model with L1 regularization, which helps to enforce sparsity by adding a penalty proportional to the absolute magnitude of the coefficients."
      ],
      "metadata": {
        "id": "rV1IUlkBsCm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "model = Lasso(alpha=0.1)\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict([[6], [7]])\n",
        "print(\"Predictions:\", y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ah1XXLOsABx",
        "outputId": "d0a0e312-e28b-4f9b-e40b-9821df83cc1c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [ 9.88 11.54]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ElasticNet:\n",
        "\n",
        "# Description: A linear regression model that combines both L1 and L2 regularization, offering a balance between Ridge and Lasso."
      ],
      "metadata": {
        "id": "4PFxthWisMlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "model = ElasticNet(alpha=0.1, l1_ratio=0.7)\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict([[6], [7]])\n",
        "print(\"Predictions:\", y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IO0ia_bpsJQt",
        "outputId": "e2ed9acc-57d6-4ac2-9287-4600902c5fc3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [ 9.85073892 11.50098522]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LogisticRegression:\n",
        "\n",
        "# Description: A linear model for binary classification that uses the logistic function to model the probability of the target variable."
      ],
      "metadata": {
        "id": "90Zyy18rsWBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Sample data for binary classification\n",
        "X = [[0.1], [0.2], [0.3], [0.4], [0.5]]\n",
        "y = [0, 0, 1, 1, 1]\n",
        "\n",
        "# Create and train the model\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict([[0.6], [0.7]])\n",
        "print(\"Predictions:\", y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA-opvacsR4q",
        "outputId": "b515c7a7-3746-4711-d67c-2146f4c09d73"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SGDRegressor and SGDClassifier:\n",
        "\n",
        "# Description: Models that use stochastic gradient descent for optimization, suitable for large-scale and sparse data."
      ],
      "metadata": {
        "id": "0jgirGQUsfnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "model = SGDRegressor()\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict([[6], [7]])\n",
        "print(\"Predictions:\", y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrOowAeAscJJ",
        "outputId": "4f2ed115-b53d-4071-8fd9-54728b0a5159"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [1.2236187  1.37254007]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "# The sklearn.linear_model module provides a range of linear models for both regression and classification tasks, each with different regularization techniques and optimization methods. These models are foundational in machine learning and are often used as benchmarks or starting points for more complex models."
      ],
      "metadata": {
        "id": "6N1sVrtwsnuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 18. What does model.fit() do? What arguments must be given?\n",
        "# The model.fit() method is a core function in many machine learning libraries, including scikit-learn. It is used to train the model on the provided training data. During this process, the model learns the patterns and relationships in the data, adjusting its parameters to minimize the loss function and improve its predictions.\n",
        "\n",
        "# What model.fit() Does:\n",
        "# Training: The fit() method trains the model using the provided input features (X) and target values (y).\n",
        "\n",
        "# Parameter Adjustment: The model's parameters (e.g., weights in linear regression) are adjusted to minimize the difference between the predicted values and the actual target values.\n",
        "\n",
        "# Optimization: It may involve optimization algorithms such as gradient descent to iteratively improve the model's performance.\n",
        "\n",
        "# Arguments Required for model.fit():\n",
        "# X (Input Features): This is the training data with the input features. It is usually provided as an array-like structure (e.g., NumPy array, pandas DataFrame).\n",
        "\n",
        "# y (Target Values): These are the true labels or target values corresponding to the input features. It should also be an array-like structure."
      ],
      "metadata": {
        "id": "uLdaIraqsvEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([1.5, 3.1, 4.9, 6.8, 8.2])\n",
        "\n",
        "# Create the model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model to the data\n",
        "model.fit(X, y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "oUVXeCtPskIK",
        "outputId": "6bb97d2c-11a7-4984-a9a6-52dba7184f25"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LinearRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LinearRegression.html\">?<span>Documentation for LinearRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LinearRegression()</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional Arguments:\n",
        "# sample_weight: Array-like of shape (n_samples,), optional. Individual weights for each sample.\n",
        "\n",
        "# callbacks (in some frameworks like Keras): Functions to call during training at the end of each epoch.\n",
        "\n",
        "# Summary:\n",
        "# The model.fit() method is essential for training machine learning models. It takes the input features (X) and target values (y) as primary arguments and adjusts the model's parameters to minimize the loss function and improve predictive performance."
      ],
      "metadata": {
        "id": "oZ6mlCX_tXPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 19. What does model.predict() do? What arguments must be given?\n",
        "# The model.predict() method in machine learning is used to make predictions on new, unseen data based on the patterns the model has learned during training. Here's a detailed explanation:\n",
        "\n",
        "# What model.predict() Does\n",
        "# Predictions: It takes the trained model and applies it to the input features provided, generating predicted values for the target variable.\n",
        "\n",
        "# Inference: This method is used during the inference phase, where the goal is to apply the model to new data to predict outcomes.\n",
        "\n",
        "# Arguments Required for model.predict()\n",
        "# X (Input Features): The primary argument required is the input features (X) on which you want to make predictions. This should be in the same format and structure as the data used for training the model (e.g., array-like, NumPy array, pandas DataFrame).\n",
        "\n"
      ],
      "metadata": {
        "id": "Jgol3fSOtifL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data for training\n",
        "X_train = np.array([[1], [2], [3], [4], [5]])\n",
        "y_train = np.array([1.5, 3.1, 4.9, 6.8, 8.2])\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# New data for prediction\n",
        "X_new = np.array([[6], [7]])\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_new)\n",
        "print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Sg4i7HOtPsg",
        "outputId": "6270b208-e974-43bc-f9e5-14d2df78ea97"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [10.03 11.74]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "# The model.predict() method is crucial for applying the trained model to new data and generating predictions. The main argument required is the input features (X), which should match the structure of the training data."
      ],
      "metadata": {
        "id": "UfrIk38rt6fl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20. What are continuous and categorical variables?\n",
        "# Continuous Variables\n",
        "# Continuous variables are numerical variables that can take an infinite number of values within a given range. They are measurable and can be divided into finer and finer levels of precision. These variables are often associated with quantities that can be measured rather than counted.\n",
        "\n",
        "# Examples:\n",
        "\n",
        "# Height: A person's height can be measured to any level of precision (e.g., 170.5 cm, 170.52 cm).\n",
        "\n",
        "# Temperature: The temperature in a room can be recorded to decimal places (e.g., 23.5°C, 23.57°C).\n",
        "\n",
        "# Characteristics:\n",
        "\n",
        "# Can take any value within a range.\n",
        "\n",
        "# Represent measurements.\n",
        "\n",
        "# Often include fractions and decimals.\n",
        "\n",
        "# Categorical Variables\n",
        "# Categorical variables, also known as qualitative variables, are variables that can take on a limited, fixed number of possible values, representing different categories or groups. They are not numerical and usually describe attributes or characteristics.\n",
        "\n",
        "# Examples:\n",
        "\n",
        "# Gender: Male, Female, Non-binary.\n",
        "\n",
        "# Color: Red, Blue, Green, Yellow.\n",
        "\n",
        "# Characteristics:\n",
        "\n",
        "# Take on discrete values.\n",
        "\n",
        "# Represent groups or categories.\n",
        "\n",
        "# Do not have an inherent order (in nominal categorical variables) or have a specific order (in ordinal categorical variables).\n",
        "\n"
      ],
      "metadata": {
        "id": "BEGW1sECuE9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'Height': [160.5, 170.2, 165.3, 180.0, 175.8],  # Continuous variable\n",
        "    'Gender': ['Male', 'Female', 'Female', 'Male', 'Non-binary'],  # Categorical variable\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saH73AeLt3dc",
        "outputId": "271d8332-c486-4996-ad4b-26aeb26099e9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Height      Gender\n",
            "0   160.5        Male\n",
            "1   170.2      Female\n",
            "2   165.3      Female\n",
            "3   180.0        Male\n",
            "4   175.8  Non-binary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "# Continuous Variables: Numerical, measurable values that can take an infinite number of values within a range.\n",
        "\n",
        "# Categorical Variables: Qualitative, countable values representing different groups or categories.\n",
        "\n",
        "# Understanding the distinction between continuous and categorical variables is crucial in machine learning, as it influences how data is preprocessed, analyzed, and used to train models."
      ],
      "metadata": {
        "id": "vVU-2qG5v7Y5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 21. What is feature scaling? How does it help in Machine Learning?\n",
        "# Feature Scaling in Machine Learning\n",
        "# Feature scaling is the process of standardizing or normalizing the range of independent variables or features in a dataset. It ensures that all features contribute equally to the model and that one feature does not dominate due to its larger scale. Here are some common techniques for feature scaling:\n",
        "\n",
        "# Standardization (Z-score normalization):\n",
        "\n",
        "# Description: Scales features to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "# Formula:\n",
        "# 𝑧=𝑥−𝜇/𝜎\n",
        "\n",
        "# 𝑥 is the feature value.\n",
        "\n",
        "# 𝜇 is the mean of the feature.\n",
        "\n",
        "# 𝜎 is the standard deviation of the feature."
      ],
      "metadata": {
        "id": "vXbM4fZawGt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = [[1, 2], [3, 4], [5, 6]]\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(scaled_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzyUbsdVvTec",
        "outputId": "15402e0b-ae01-4c16-b1f8-3bb308e5ec2f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalization (Min-Max scaling):\n",
        "\n",
        "# Description: Scales features to a fixed range, typically [0, 1].\n",
        "\n",
        "# Formula: 𝑥′=𝑥−𝑥min/𝑥max−𝑥min\n",
        "\n",
        "# 𝑥 is the feature value.\n",
        "\n",
        "# 𝑥 min is the minimum value of the feature.\n",
        "\n",
        "# 𝑥 max is the maximum value of the feature."
      ],
      "metadata": {
        "id": "6UptEfiIxDYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "data = [[1, 2], [3, 4], [5, 6]]\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(scaled_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWk9r-50w8pi",
        "outputId": "c58e4755-6983-4609-8fe8-3e4eede43275"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.  0. ]\n",
            " [0.5 0.5]\n",
            " [1.  1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Robust Scaling:\n",
        "\n",
        "# Description: Scales features using statistics that are robust to outliers.\n",
        "\n",
        "# Formula: 𝑥′=𝑥−𝑄1/𝑄3−𝑄1\n",
        "\n",
        "# 𝑄1 is the first quartile.\n",
        "\n",
        "# 𝑄3 is the third quartile."
      ],
      "metadata": {
        "id": "z9Z0oFFixtk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "data = [[1, 2], [3, 4], [5, 6]]\n",
        "scaler = RobustScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(scaled_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xHgM0HXxf0f",
        "outputId": "3bd74ef3-7140-4db2-f44b-def33aaa22a0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1. -1.]\n",
            " [ 0.  0.]\n",
            " [ 1.  1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importance of Feature Scaling\n",
        "# Improves Convergence Speed:\n",
        "\n",
        "# Gradient Descent: Many machine learning algorithms, especially those using gradient descent, converge faster when features are on a similar scale.\n",
        "\n",
        "# Prevents Dominance by Large-Scale Features:\n",
        "\n",
        "# Balanced Contributions: Ensures that all features contribute equally to the model and that one feature does not dominate due to its larger scale.\n",
        "\n",
        "# Improves Model Performance:\n",
        "\n",
        "# Distance-Based Algorithms: Algorithms such as K-Nearest Neighbors (KNN) and Support Vector Machines (SVM) rely on distance calculations and perform better when features are scaled.\n",
        "\n",
        "# Enhanced Interpretability:\n",
        "\n",
        "# Consistent Scale: Makes it easier to interpret the impact of features on the model's predictions.\n",
        "\n",
        "# Summary\n",
        "# Feature scaling is a crucial preprocessing step in machine learning that ensures all features contribute equally to the model. Techniques such as standardization, normalization, and robust scaling help improve model performance, convergence speed, and interpretability."
      ],
      "metadata": {
        "id": "bW-I8kiQyS2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 22. How do we perform scaling in Python?\n",
        "# Performing feature scaling in Python is straightforward using the scikit-learn library. Here are some common techniques to scale features:\n",
        "\n",
        "# 1. Standardization (Z-score Normalization)\n",
        "# Standardization scales features to have a mean of 0 and a standard deviation of 1."
      ],
      "metadata": {
        "id": "xRBvFzRVy13t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Sample data\n",
        "data = [[1, 2], [3, 4], [5, 6]]\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(\"Standardized Data:\\n\", scaled_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1bbHsNtyHQj",
        "outputId": "66e56c3d-2080-4ce4-c936-febdac750529"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardized Data:\n",
            " [[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Normalization (Min-Max Scaling)\n",
        "# Normalization scales features to a fixed range, typically between 0 and 1."
      ],
      "metadata": {
        "id": "F0nc8gAUzJey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Sample data\n",
        "data = [[1, 2], [3, 4], [5, 6]]\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(\"Normalized Data:\\n\", scaled_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mk_YCyHJzFXZ",
        "outputId": "c1719367-1c49-49e7-be0b-5eb9b65a45f7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized Data:\n",
            " [[0.  0. ]\n",
            " [0.5 0.5]\n",
            " [1.  1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Robust Scaling\n",
        "# Robust scaling uses statistics that are robust to outliers (e.g., median and interquartile range)."
      ],
      "metadata": {
        "id": "9WRWGnrOzUfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Sample data\n",
        "data = [[1, 2], [3, 4], [5, 6]]\n",
        "scaler = RobustScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(\"Robustly Scaled Data:\\n\", scaled_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1A8rIJ0PzP8g",
        "outputId": "65cdf14c-b97f-46b9-b7c9-d0fef4dde16b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Robustly Scaled Data:\n",
            " [[-1. -1.]\n",
            " [ 0.  0.]\n",
            " [ 1.  1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "# Standardization: Useful when features follow a Gaussian distribution.\n",
        "\n",
        "# Normalization: Useful when you need the features to be within a specific range.\n",
        "\n",
        "# Robust Scaling: Useful when dealing with outliers.\n",
        "\n",
        "# By using these techniques, you can ensure that all features contribute equally to the model, improving its performance and convergence speed."
      ],
      "metadata": {
        "id": "Yq8z7cEGzesV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 23. What is sklearn.preprocessing?\n",
        "# sklearn.preprocessing is a module within the scikit-learn library in Python that provides a variety of functions and classes for preprocessing data before it is fed into machine learning models. Preprocessing is a crucial step in the machine learning pipeline, as it involves transforming raw data into a suitable format that can improve the performance and accuracy of models.\n",
        "# Key Functions and Classes in sklearn.preprocessing\n",
        "# Standardization and Normalization:\n",
        "\n",
        "# StandardScaler: Standardizes features by removing the mean and scaling to unit variance.\n",
        "\n",
        "# MinMaxScaler: Scales features to a given range, typically between 0 and 1.\n",
        "\n",
        "# Normalizer: Normalizes samples individually to unit norm (e.g., L2 norm).\n",
        "\n",
        "# Encoding Categorical Features:\n",
        "\n",
        "# LabelEncoder: Converts categorical labels into integer codes.\n",
        "\n",
        "# OneHotEncoder: Encodes categorical integer features as a one-hot numeric array.\n",
        "\n",
        "# Binarization:\n",
        "\n",
        "# Binarizer: Converts continuous data into binary values based on a threshold.\n",
        "\n",
        "# Polynomial Features:\n",
        "\n",
        "# PolynomialFeatures: Generates polynomial and interaction features from the original features.\n",
        "\n",
        "# Handling Missing Values:\n",
        "\n",
        "# SimpleImputer: Fills in missing values using specified strategies (e.g., mean, median, most frequent).\n",
        "\n",
        "# Feature Scaling:\n",
        "\n",
        "# RobustScaler: Scales features using statistics that are robust to outliers."
      ],
      "metadata": {
        "id": "c-C6kAbezynL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "# The sklearn.preprocessing module in scikit-learn provides essential tools for transforming and preparing data for machine learning models. By standardizing, encoding, binarizing, and generating new features, you can enhance the quality of your dataset and improve model performance."
      ],
      "metadata": {
        "id": "rB-vblqD0nib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 24. How do we split data for model fitting (training and testing) in Python?\n",
        "# To split data for model fitting (training and testing) in Python, you can use the train_test_split function from the sklearn.model_selection module in scikit-learn. This function helps split your dataset into training and test sets efficiently, ensuring that your model is trained on one portion of the data and evaluated on another. This process helps to assess the model's performance and its ability to generalize to unseen data.\n",
        "# Explanation:\n",
        "# X (Input Features): The feature matrix, which contains the input data.\n",
        "\n",
        "# y (Target Variable): The target variable or labels corresponding to the input features.\n",
        "\n",
        "# test_size: The proportion of the dataset to include in the test split (e.g., test_size=0.2 means 20% of the data is used for testing).\n",
        "\n",
        "# random_state: A seed value used by the random number generator to ensure reproducibility of the split.\n",
        "\n",
        "# Summary:\n",
        "# The train_test_split function is used to split the data into training and test sets.\n",
        "\n",
        "# The training set is used to train the model, while the test set is used to evaluate its performance.\n",
        "\n",
        "# Specifying test_size determines the proportion of the data used for testing.\n",
        "\n",
        "# Using random_state ensures that the split is reproducible.\n",
        "\n",
        "# By properly splitting your data, you can effectively train and evaluate your machine learning models, ensuring that they perform well on unseen data."
      ],
      "metadata": {
        "id": "_Ugf5ihj0xYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
        "y = np.array([1, 2, 3, 4, 5])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Output the results\n",
        "print(\"X_train:\\n\", X_train)\n",
        "print(\"X_test:\\n\", X_test)\n",
        "print(\"y_train:\\n\", y_train)\n",
        "print(\"y_test:\\n\", y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej6o2WAT1kcV",
        "outputId": "d138807f-68ff-452a-af69-93fe4e9cef5e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train:\n",
            " [[ 9 10]\n",
            " [ 5  6]\n",
            " [ 1  2]\n",
            " [ 7  8]]\n",
            "X_test:\n",
            " [[3 4]]\n",
            "y_train:\n",
            " [5 3 1 4]\n",
            "y_test:\n",
            " [2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 25. Explain data encoding?\n",
        "# Data Encoding in Machine Learning\n",
        "# Data encoding is the process of converting categorical data into numerical formats so that it can be used effectively by machine learning algorithms. Most algorithms require numerical input, so encoding is a crucial step in data preprocessing. Here are some common techniques for data encoding:\n",
        "\n",
        "# 1. Label Encoding\n",
        "# Description: Converts each category value into a unique integer. This is suitable for ordinal variables, where the order matters.\n",
        "\n"
      ],
      "metadata": {
        "id": "bERXcOG81aBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Sample data\n",
        "categories = ['low', 'medium', 'high']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(categories)\n",
        "print(\"Label Encoded:\", encoded_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "As0rpoKO0f1M",
        "outputId": "8e00dad7-2f0d-4e8b-ac0a-e6799d38625d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Encoded: [1 2 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. One-Hot Encoding\n",
        "# Description: Converts each category value into a new binary column (0 or 1). This is useful when the categorical variable does not have an inherent order (nominal variables)."
      ],
      "metadata": {
        "id": "69fLUYZw192r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = {'Color': ['Red', 'Blue', 'Green']}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "one_hot_encoded = pd.get_dummies(df, columns=['Color'])\n",
        "print(\"One-Hot Encoded:\\n\", one_hot_encoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upXi6DT0155v",
        "outputId": "8564d2a3-c8d3-42fe-e533-69dcca1d64b2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-Hot Encoded:\n",
            "    Color_Blue  Color_Green  Color_Red\n",
            "0       False        False       True\n",
            "1        True        False      False\n",
            "2       False         True      False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Binary Encoding\n",
        "# Description: Combines the properties of label encoding and one-hot encoding. It first assigns a unique integer to each category, then converts those integers into binary code, and creates separate columns for each bit."
      ],
      "metadata": {
        "id": "rSTiOaSc2F-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Frequency Encoding\n",
        "# Description: Replaces each category with the frequency of its occurrence in the dataset. This is useful for high cardinality categorical variables."
      ],
      "metadata": {
        "id": "_uLZJmNd2SlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = {'Category': ['A', 'B', 'A', 'C', 'B', 'A']}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "frequency_encoded = df['Category'].map(df['Category'].value_counts())\n",
        "print(\"Frequency Encoded:\\n\", frequency_encoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QsC9mnA2KjF",
        "outputId": "426a0131-d6c5-4e8d-bcdd-21a95af281b5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency Encoded:\n",
            " 0    3\n",
            "1    2\n",
            "2    3\n",
            "3    1\n",
            "4    2\n",
            "5    3\n",
            "Name: Category, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Target Encoding\n",
        "# Description: Replaces each category with the average value of the target variable for that category. This technique is useful for avoiding high-dimensionality problems and is typically used in supervised learning tasks."
      ],
      "metadata": {
        "id": "aPi3McOI2c4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = {'Category': ['A', 'B', 'A', 'C', 'B', 'A'],\n",
        "        'Target': [10, 20, 30, 40, 20, 30]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate target mean for each category\n",
        "target_mean = df.groupby('Category')['Target'].mean()\n",
        "target_encoded = df['Category'].map(target_mean)\n",
        "print(\"Target Encoded:\\n\", target_encoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IqKJh3w2Y6u",
        "outputId": "306c8ccc-44c8-4e33-d3d9-268731d26a4e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target Encoded:\n",
            " 0    23.333333\n",
            "1    20.000000\n",
            "2    23.333333\n",
            "3    40.000000\n",
            "4    20.000000\n",
            "5    23.333333\n",
            "Name: Category, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "# Data encoding is essential for preparing categorical data for machine learning models. Techniques like label encoding, one-hot encoding, binary encoding, frequency encoding, and target encoding each serve different purposes based on the nature of the data and the specific use case. Proper encoding ensures that the categorical variables can be effectively utilized by machine learning algorithms."
      ],
      "metadata": {
        "id": "EGkvFNKR2i4w"
      }
    }
  ]
}